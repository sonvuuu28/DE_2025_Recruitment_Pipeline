# **DE project: Recruitment Pipeline**

[![Python](https://img.shields.io/badge/python-3.10-blue)](https://www.python.org/)
[![Spark](https://img.shields.io/badge/spark-3.5.6-orange)](https://spark.apache.org/)
[![Cassandra](https://img.shields.io/badge/Cassandra-latest-red)](https://cassandra.apache.org/)
[![MySQL](https://img.shields.io/badge/MySQL-latest-lightgrey)](https://www.mysql.com/)
[![Grafana](https://img.shields.io/badge/Grafana-latest-orange)](https://grafana.com/) 
[![Docker](https://img.shields.io/badge/Docker-latest-blue)](https://www.docker.com/) 
[![GitHub](https://img.shields.io/badge/GitHub-latest-black)](https://github.com/)
---

# Má»¥c Lá»¥c

1. [Tá»•ng quan dá»± Ã¡n](#tá»•ng-quan-dá»±-Ã¡n)
2. [I. Docker Preparation](#i-docker-preparation)
3. [II. ETL Pipeline](#ii-etl-pipeline)
4. [III. Visualization (Grafana)](#iii-visualization-grafana)
5. [IV. Server Preparation](#iv-server-preparation)
6. [V. Deployment & CI/CD](#v-deployment--cicd)
---
# Tá»•ng quan dá»± Ã¡n
### 1. Má»¥c tiÃªu dá»± Ã¡n
XÃ¢y pipeline Micro-Batch ETL near-real-time tá»« CSV tÄ©nh vÃ  API giáº£ láº­p CDC: lÆ°u dá»¯ liá»‡u thÃ´ vÃ o Cassandra(Data Lake), transform báº±ng Spark + Python, load vÃ o MySQL(Data Warehouse) vÃ  hiá»ƒn thá»‹ trÃªn Grafana.  ToÃ n bá»™ há»‡ thá»‘ng Ä‘Æ°á»£c container hÃ³a báº±ng Docker vÃ  triá»ƒn khai trÃªn má»™t mÃ¡y áº£o VirtualBox, kÃ¨m CI/CD trÃªn GitHub.

---
### 2. High Level Architecture
Dá»¯ liá»‡u Ä‘i tá»« Sources â†’ Ingest â†’ Data Lake (Cassandra) â†’ ETL (Spark) â†’ Data Warehouse (MySQL) â†’ Consumers (Grafana / SQL). Táº¥t cáº£ components Ä‘á»u Ä‘Æ°á»£c Ä‘Ã³ng gÃ³i báº±ng docker vÃ  cháº¡y trong VM.
![alt text](image/hla.png)

---
### 3. Input/Output
**Input**

* File CSV tracking.
* API Python sinh sá»± kiá»‡n (giáº£ láº­p CDC). Sanh ra báº£n ghi tÆ°Æ¡ng á»©ng metadata cá»§a tracking.

| Column Name    | Description |
|----------------|-------------|
| create_time    | Thá»i Ä‘iá»ƒm event Ä‘Æ°á»£c táº¡o (UUID v1) |
| job_id         | ID cÃ´ng viá»‡c liÃªn quan Ä‘áº¿n event |
| custom_track   | Loáº¡i event: click, conversion, qualified, unqualified |
| bid            | GiÃ¡ tháº§u (bid) cho event |
| campaign_id    | ID chiáº¿n dá»‹ch quáº£ng cÃ¡o |
| group_id       | ID nhÃ³m |
| publisher_id   | ID nhÃ  xuáº¥t báº£n |


**Output**

* Dashboard phÃ¢n tÃ­ch trÃªn Grafana
* Pipeline ETL micro-batch tá»± Ä‘á»™ng
* Táº¥t cáº£ cháº¡y trÃªn single VM VirtualBox
* Flat Table output:

| Column Name    | Description |
|----------------|-------------|
| id             | ID báº£n ghi tá»± sinh (unique) |
| job_id         | ID cÃ´ng viá»‡c liÃªn quan Ä‘áº¿n event |
| dates          | NgÃ y xáº£y ra event (YYYY-MM-DD) |
| hours          | Giá» trong ngÃ y khi event xáº£y ra (0-23) |
| company_id     | ID cÃ´ng ty sá»Ÿ há»¯u job/campaign |
| group_id       | ID nhÃ³m liÃªn quan Ä‘áº¿n job/campaign |
| campaign_id    | ID chiáº¿n dá»‹ch quáº£ng cÃ¡o |
| publisher_id   | ID nhÃ  xuáº¥t báº£n |
| click          | Sá»‘ láº§n click |
| conversion     | Sá»‘ láº§n chuyá»ƒn Ä‘á»•i |
| qualified      | Sá»‘ lÆ°á»£t qualified |
| unqualified    | Sá»‘ lÆ°á»£t unqualified |
| bid_set        | GiÃ¡ tháº§u trung bÃ¬nh |
| spend_hour     | Tá»•ng chi tiÃªu theo giá» |
| sources        | Nguá»“n dá»¯ liá»‡u (vÃ­ dá»¥: Cassandra) |
| updated_at     | Thá»i Ä‘iá»ƒm báº£n ghi Ä‘Æ°á»£c cáº­p nháº­t |

---
### 4. Demo
`ETL cháº¡y tá»± Ä‘á»™ng liÃªn tá»¥c`

![ETL Demo](image/demo_etl.gif)

---
`Grafana Dashboard`

![alt text](image/grafana.png)

---
### Káº¿t quáº£ trÃªn server

`Cassandra (Data Lake) lÆ°u dá»¯ liá»‡u thÃ´`

![alt text](image/output_cassandra.png)

---

`MySQL (Data Warehouse) lÆ°u dá»¯ liá»‡u Ä‘Ã£ transform`

![alt text](image/output_mysql.png)

---

`Spark Engine xá»­ lÃ½ micro-batch liÃªn tá»¥c`

![alt text](image/demo_server.gif)



----
# I. Docker Preparation

```
docker/
â”œâ”€â”€ ğŸ“ config
â”‚   â””â”€â”€ âš™ï¸ spark-defaults.conf
â”œâ”€â”€ ğŸ“ spark
â”‚   â””â”€â”€ ğŸ³ Dockerfile
â”œâ”€â”€ ğŸ“„ .env
â”œâ”€â”€ âš™ï¸ docker-compose.yml
â”œâ”€â”€ ğŸ“„ entrypoint.sh
â””â”€â”€ ğŸ“„ requirements.txt
```

Má»¥c tiÃªu: Build image Spark riÃªng, Cassandra vÃ  MySQL dÃ¹ng image tá»« Docker Hub.

---

## 1. Build Spark Image

Dockerfile:

```dockerfile
FROM python:3.10-bookworm as spark-base

# CÃ i tool cáº§n thiáº¿t
RUN apt-get update && \
    apt-get install -y sudo curl vim unzip rsync openjdk-17-jdk build-essential software-properties-common ssh && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Thiáº¿t láº­p mÃ´i trÆ°á»ng
ENV SPARK_HOME=/opt/spark
ENV HADOOP_HOME=/opt/hadoop
RUN mkdir -p ${SPARK_HOME} ${HADOOP_HOME}
WORKDIR ${SPARK_HOME}

# Táº£i Spark
RUN curl https://archive.apache.org/dist/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz -o spark.tgz \
 && tar xvzf spark.tgz --strip-components 1 \
 && rm spark.tgz

# CÃ i Python dependencies
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# Config Spark
ENV PATH="$SPARK_HOME/sbin:$SPARK_HOME/bin:$PATH"
ENV SPARK_MASTER="spark://spark-master:7077"
ENV PYSPARK_PYTHON=python3
COPY config/spark-defaults.conf $SPARK_HOME/config
RUN chmod +x $SPARK_HOME/sbin/* $SPARK_HOME/bin/*

ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH

# Entrypoint
COPY entrypoint.sh .
ENTRYPOINT ["./entrypoint.sh"]
```

LÆ°u Ã½:

* Spark cháº¡y trong container, káº¿t ná»‘i Cassandra / MySQL tá»« Docker Hub.
* `entrypoint.sh` cháº¡y ETL tá»± Ä‘á»™ng khi container start.

---

## 2. Docker Compose Setup
docker-compose:
```yaml
services:

  # Cassandra (Data Lake)
  cassandra:
    image: cassandra:4.1
    container_name: cassandra_dl
    ports:
      - "9042:9042"
    volumes:
      - cassandra-data:/var/lib/cassandra
    environment:
      CASSANDRA_CLUSTER_NAME: "first_cluster"
    healthcheck:
      test: ["CMD-SHELL", "cqlsh -e 'describe keyspaces'"]
      interval: 10s
      retries: 5
    networks:
      - de_project

  # MySQL (Data Warehouse)
  mysql:
    image: mysql:8.0.44-debian
    container_name: mysql_dwh
    ports:
      - "3307:3306"
    environment:
      MYSQL_ROOT_PASSWORD: 123
    volumes:
      - mysql-data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      retries: 5
    networks:
      - de_project

  # Spark Master
  spark-master:
    container_name: spark-engine
    build:
      context: .
      dockerfile: spark/Dockerfile
    image: da-spark-image
    entrypoint: ['./entrypoint.sh', 'master']
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - ../data:/opt/spark/data
      - ../etl:/opt/spark/etl
      - spark-logs:/opt/spark/spark-events
    env_file:
      - .env
    ports:
      - "9090:8080"  # Web UI
      - "7077:7077"  # Spark master
      - "4041:4040"  # Spark driver UI
    networks:
      - de_project

  # Grafana (Monitoring)
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    networks:
      - de_project

# Volumes
volumes:
  cassandra-data:
  mysql-data:
  spark-logs:

# Networks
networks:
  de_project:
    name: de_project
    driver: bridge
```

Giáº£i thÃ­ch:

* Cassandra â†’ Data Lake, port 9042
* MySQL â†’ Data Warehouse, port 3307
* Spark Master â†’ cháº¡y ETL, káº¿t ná»‘i CSV/ETL code
* Grafana â†’ Monitoring, port 3000
* Volumes â†’ lÆ°u dá»¯ liá»‡u persistent
* Network de_project â†’ táº¥t cáº£ container cÃ¹ng network ná»™i bá»™

---

# II. ETL Pipeline
```
â”œâ”€â”€ ğŸ Cassandra.py
â”œâ”€â”€ ğŸ Main.py
â”œâ”€â”€ ğŸ MySql.py
â””â”€â”€ ğŸ generate_data_automatically.py
```
Má»¥c tiÃªu:
- Láº¥y dá»¯ liá»‡u thÃ´ tá»« Cassandra (Datalake)
- Transfrom á»Ÿ Main
- ÄÆ°a dá»¯ liá»‡u vÃ o MySQL (Data Warehouse)
- Táº¡o cÃ¡c báº£n ghi liÃªn tá»¥c tá»± Ä‘á»™ng Ä‘Æ°a vÃ o Datalake (ğŸ generate_data_automatically.py)

### 1. Main
```python
import os
from uuid import UUID
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import StringType
from cassandra.util import datetime_from_uuid1

from Cassandra import Cassandra
from MySql import MySql

# ===========================================================
# SPARK CONFIG â€” káº¿t ná»‘i Cassandra + MySQL cho Spark
# ===========================================================

MYSQL_JAR = os.path.abspath("../driver/mysql-connector-j-8.0.33.jar")

spark = (
    SparkSession.builder.config(
        "spark.jars.packages", "com.datastax.spark:spark-cassandra-connector_2.12:3.1.0"
    )
    .config("spark.cassandra.connection.host", "cassandra_dl")
    .config("spark.cassandra.connection.port", "9042")
    .config("spark.driver.extraClassPath", MYSQL_JAR)
    .config("spark.executor.extraClassPath", MYSQL_JAR)
    .getOrCreate()
)

# DB wrappers
cass = Cassandra(spark)
mysql = MySql(spark)

# ===========================================================
# UDF â€” chuyá»ƒn UUID v1 â†’ timestamp
# ===========================================================

@udf(returnType=StringType())
def extract_timestamp_from_uuid(uuid_str):
    try:
        u = UUID(uuid_str)
        dt = datetime_from_uuid1(u)
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except:
        return None


# ===========================================================
# Utility Ä‘á»c file CSV
# ===========================================================

def spark_read_file(path):
    base_dir = os.path.dirname(os.path.abspath(__file__))
    return spark.read.csv(os.path.join(base_dir, path), header=True)


# ===========================================================
# DATA TRANSFORM PIPELINE â€” nÆ¡i xá»­ lÃ½ chÃ­nh
# ===========================================================

class DataTransformer:

    # CÃ¡c event há»£p lá»‡ Ä‘á»ƒ pivot
    valid_events = ["click", "conversion", "qualified", "unqualified"]

    @staticmethod
    def preprocess(df):
        # Convert UUID â†’ timestamp
        df = df.withColumn("system_ts", extract_timestamp_from_uuid(col("create_time")))
        df = df.withColumn("system_ts", to_timestamp("system_ts"))

        # Chá»n cá»™t cáº§n thiáº¿t + lá»c null
        df = df.select(
            "create_time", "system_ts", "job_id", "custom_track",
            "bid", "campaign_id", "group_id", "publisher_id"
        ).filter("job_id IS NOT NULL AND custom_track IS NOT NULL")

        return df

    @staticmethod
    def aggregate(df):
        # Giá»¯ event há»£p lá»‡
        df = df.filter(col("custom_track").isin(DataTransformer.valid_events))

        # TÃ¡ch ngÃ y & giá»
        df = df.withColumn("dates", to_date("system_ts"))
        df = df.withColumn("hours", hour("system_ts"))

        # Pivot event â†’ Ä‘áº¿m sá»‘ láº§n click / conversion / ...
        pivot_df = (
            df.groupBy("job_id", "dates", "hours",
                       "publisher_id", "campaign_id", "group_id")
            .pivot("custom_track", DataTransformer.valid_events)
            .agg(count("*").alias("count"))
        )

        # Rename: click_count â†’ click
        for e in DataTransformer.valid_events:
            pivot_df = pivot_df.withColumnRenamed(f"{e}_count", e)

        # TÃ­nh spend + bid trung bÃ¬nh theo job/group
        metric_df = df.groupBy("job_id", "publisher_id", "campaign_id", "group_id").agg(
            round(sum("bid"), 2).alias("spend_hour"),
            round(avg("bid"), 2).alias("bid_set"),
        )

        # Join pivot + metrics
        return pivot_df.join(metric_df,
                             ["job_id", "publisher_id", "campaign_id", "group_id"],
                             "left")

    @staticmethod
    def fill_null(df):
        # Äiá»n 0 cho táº¥t cáº£ metric
        fill_values = {
            "click": 0, "conversion": 0, "qualified": 0,
            "unqualified": 0, "spend_hour": 0, "bid_set": 0,
        }
        return df.fillna(fill_values)

    @staticmethod
    def post_process(df):
        # Primary key tá»± sinh
        df = df.withColumn("id", monotonically_increasing_id())

        # Timestamp cáº­p nháº­t
        df = df.withColumn("updated_at", current_timestamp())

        # Chá»n vÃ  sáº¯p xáº¿p cá»™t
        return df.select(
            "id", "job_id", "dates", "hours",
            "company_id", "group_id", "campaign_id", "publisher_id",
            "click", "conversion", "qualified", "unqualified",
            "bid_set", "spend_hour", "sources", "updated_at",
        )

    @staticmethod
    def transform_full(df):
        df = DataTransformer.preprocess(df)
        df = DataTransformer.aggregate(df)
        df = df.withColumn("sources", lit("Cassandra"))
        df = DataTransformer.fill_null(df)

        # Join thÃªm company tá»« MySQL
        job_df = mysql.read("job").select(col("id").alias("job_id"), "company_id")
        df = df.join(job_df, "job_id", "left")

        return DataTransformer.post_process(df)


# ===========================================================
# SYNC CHECK â€” kiá»ƒm tra cÃ³ dá»¯ liá»‡u má»›i Ä‘á»ƒ ETL tiáº¿p khÃ´ng
# ===========================================================

class DataSync:

    @staticmethod
    def last_mysql_date():
        df = mysql.read("event")
        return df.select(max("updated_at")).first()[0]

    @staticmethod
    def last_cassandra_date():
        df = cass.read("tracking")

        # Convert create_time trong Cassandra â†’ timestamp VN timezone
        df = df.withColumn("create_time", extract_timestamp_from_uuid("create_time"))
        df = df.withColumn("create_time", to_timestamp("create_time"))
        df = df.withColumn("create_time",
            from_utc_timestamp("create_time", "Asia/Ho_Chi_Minh"))

        return df.select(max("create_time")).first()[0]


# ===========================================================
# MAIN ETL â€” cháº¡y toÃ n pipeline
# ===========================================================

def run_etl():
    df = cass.read("tracking")
    df = DataTransformer.transform_full(df)
    mysql.insert("event", df)


# ===========================================================
# ENTRY POINT â€” cháº¡y láº§n Ä‘áº§u vÃ  sync liÃªn tá»¥c
# ===========================================================

if __name__ == "__main__":
    # Load dá»¯ liá»‡u thÃ´ ban Ä‘áº§u
    print("Insert Cassandra")
    cass.insert("tracking", spark_read_file("../data/cassandra/tracking.csv"))

    print("Insert MySQL")
    mysql.insert("job", spark_read_file("../data/mysql/job.csv"))

    # Cháº¡y ETL láº§n Ä‘áº§u
    run_etl()

    # Loop sync â€” náº¿u Cassandra cÃ³ dá»¯ liá»‡u má»›i â†’ ETL láº¡i
    while True:
        if DataSync.last_mysql_date() < DataSync.last_cassandra_date():
            run_etl()
```
### 2. Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c 
![ETL Demo](image/demo_etl.gif)

**Nháº­n xÃ©t:**
- Dá»¯ liá»‡u Ä‘Æ°á»£c xá»­ lÃ½ tá»± Ä‘á»™ng khi cÃ³ báº£n ghi má»›i Ä‘Æ°á»£c Ä‘Æ°a vÃ o Datalake: Micro-Batch ETL

---
# III. Visualization (Grafana)
### 1. Config Mysql
```
Grafana â†’ Connections â†’ Data Sources â†’ mysql 
```

![Grafana Config](image/grafana_config.png)

### 2. Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c
![alt text](image/grafana.png)

---
# IV. Server Preparation

### 1. CÃ i Ä‘áº·t VM

* CÃ i VirtualBox: [VirtualBox](https://www.virtualbox.org/wiki/Downloads)
* Táº£i Ubuntu Server: [Ubuntu](https://ubuntu.com/download/server)

---

### 2. Cáº¥u hÃ¬nh server

#### 2.1 Táº¡o mÃ¡y áº£o

1. VirtualBox â†’ New â†’ Name: `Ubuntu_VM`
2. Type: Linux, Version: Ubuntu (64-bit)
3. RAM: 2â€“4 GB
4. Hard disk: 20GB+
5. Start VM â†’ cÃ i Ubuntu tá»« ISO

#### 2.2 Cáº¥u hÃ¬nh máº¡ng

![Config Image](image/image.png)

* Settings â†’ Network â†’ Adapter 1 â†’ Port Forwarding
* Má»Ÿ port SSH host â†’ VM (vÃ­ dá»¥ host port 2222 â†’ guest port 22)

#### 2.3 CÃ i OpenSSH server

```bash
sudo apt update
sudo apt install openssh-server -y
sudo systemctl enable ssh
sudo systemctl start ssh
```

* Kiá»ƒm tra SSH tá»« host:

```bash
ssh <username>@<host_ip> -p <host_port>
```

![test\_ssh.png](image/test_ssh.png)

#### 2.4 CÃ i Ä‘áº·t thÃªm cÃ¡c gÃ³i há»— trá»£

Trong bÆ°á»›c nÃ y, chÃºng ta sáº½ cÃ i Ä‘áº·t nhá»¯ng cÃ´ng cá»¥ cáº§n thiáº¿t Ä‘á»ƒ cháº¡y project: **Git**, **Docker Engine**, vÃ  **Docker Compose v2**.

1. CÃ i Ä‘áº·t cÃ´ng cá»¥ cÆ¡ báº£n

```bash
sudo apt update
sudo apt install -y git ca-certificates curl gnupg
```

**Giáº£i thÃ­ch:**

* `git`: dÃ¹ng Ä‘á»ƒ clone mÃ£ nguá»“n tá»« GitHub
* `curl`: dÃ¹ng Ä‘á»ƒ táº£i file tá»« internet
* `gnupg`: dÃ¹ng Ä‘á»ƒ xÃ¡c thá»±c chá»¯ kÃ½ GPG
* `ca-certificates`: Ä‘áº£m báº£o káº¿t ná»‘i HTTPS an toÃ n

2. ThÃªm Docker GPG Key

```bash
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --batch --yes --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg
```

**Ghi chÃº:**

* Docker yÃªu cáº§u **key GPG** Ä‘á»ƒ Ä‘áº£m báº£o package táº£i vá» lÃ  tháº­t, khÃ´ng bá»‹ sá»­a Ä‘á»•i.
* File key Ä‘Æ°á»£c lÆ°u trong `/etc/apt/keyrings` (chuáº©n má»›i cá»§a Ubuntu).

3. ThÃªm Docker repository vÃ o há»‡ thá»‘ng

```bash
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo $VERSION_CODENAME) stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
```

4. CÃ i Docker Engine + Docker Compose v2

```bash
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
```

---
# V. Deployment & CI/CD Pipeline
Má»¥c Ä‘Ã­ch: kÃ©o code tá»« github vá» server vÃ  cháº¡y á»Ÿ server

```
git clone https://github.com/sonvuuu28/DE_2025_Recruitment_Pipeline.git
```
![git clone](image/gitclone.png)

```
sudo docker-compose up -d
```

![alt text](image/ketqua_docker.png)
