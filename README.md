DE project: Recruitment Pipeline

[![Python](https://img.shields.io/badge/python-3.10-blue)](https://www.python.org/)  
[![Spark](https://img.shields.io/badge/spark-3.5.6-orange)](https://spark.apache.org/)  
[![Cassandra](https://img.shields.io/badge/Cassandra-latest-red)](https://cassandra.apache.org/)  
[![MySQL](https://img.shields.io/badge/MySQL-latest-lightgrey)](https://www.mysql.com/)  
[![Grafana](https://img.shields.io/badge/Grafana-latest-orange)](https://grafana.com/)  
[![Docker](https://img.shields.io/badge/Docker-latest-blue)](https://www.docker.com/)  
[![GitHub](https://img.shields.io/badge/GitHub-latest-black)](https://github.com/)

# T·ªïng quan d·ª± √°n
### 1. M·ª•c ti√™u d·ª± √°n
X√¢y pipeline Micro-Batch ETL near-real-time t·ª´ CSV tƒ©nh v√† API gi·∫£ l·∫≠p CDC: l∆∞u d·ªØ li·ªáu th√¥ v√†o Cassandra(Data Lake), transform b·∫±ng Spark + Python, load v√†o MySQL(Data Warehouse) v√† hi·ªÉn th·ªã tr√™n Grafana.  To√†n b·ªô h·ªá th·ªëng ƒë∆∞·ª£c container h√≥a b·∫±ng Docker v√† tri·ªÉn khai tr√™n m·ªôt m√°y ·∫£o VirtualBox, k√®m CI/CD tr√™n GitHub.
### 2. High Level Architecture
D·ªØ li·ªáu ƒëi t·ª´ Sources ‚Üí Ingest ‚Üí Data Lake (Cassandra) ‚Üí ETL (Spark) ‚Üí Data Warehouse (MySQL) ‚Üí Consumers (Grafana / SQL / Jupyter). T·∫•t c·∫£ components ƒë·ªÅu ƒë∆∞·ª£c ƒë√≥ng g√≥i b·∫±ng docker v√† ch·∫°y trong VM ƒë·ªÉdemo.
![alt text](image/hla.png)

### 3. Input/Output
Input Data: CSV, API python
Output: DashBoard (Grafana), v√† 1 pipeline t·ª± ƒë·ªông ETL, t·∫•t c·∫£ ch·∫°y ho√†n to√†n ·ªü 1 m√°y ·∫£o

Grafana:
![alt text](image/grafana.png)
---

Automatically ETL:
![ETL Demo](image/demo_etl.gif)

Server:


----
# I. Docker Preparation

```
docker/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ spark-defaults.conf
‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ entrypoint.sh
‚îî‚îÄ‚îÄ requirements.txt
```

M·ª•c ti√™u: Build image Spark ri√™ng, Cassandra v√† MySQL d√πng image t·ª´ Docker Hub.

---

## 1. Build Spark Image

Dockerfile:

```dockerfile
FROM python:3.10-bookworm as spark-base

# C√†i tool c·∫ßn thi·∫øt
RUN apt-get update && \
    apt-get install -y sudo curl vim unzip rsync openjdk-17-jdk build-essential software-properties-common ssh && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng
ENV SPARK_HOME=/opt/spark
ENV HADOOP_HOME=/opt/hadoop
RUN mkdir -p ${SPARK_HOME} ${HADOOP_HOME}
WORKDIR ${SPARK_HOME}

# T·∫£i Spark
RUN curl https://archive.apache.org/dist/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz -o spark.tgz \
 && tar xvzf spark.tgz --strip-components 1 \
 && rm spark.tgz

# C√†i Python dependencies
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# Config Spark
ENV PATH="$SPARK_HOME/sbin:$SPARK_HOME/bin:$PATH"
ENV SPARK_MASTER="spark://spark-master:7077"
ENV PYSPARK_PYTHON=python3
COPY config/spark-defaults.conf $SPARK_HOME/config
RUN chmod +x $SPARK_HOME/sbin/* $SPARK_HOME/bin/*

ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH

# Entrypoint
COPY entrypoint.sh .
ENTRYPOINT ["./entrypoint.sh"]
```

L∆∞u √Ω:

* Spark ch·∫°y trong container, k·∫øt n·ªëi Cassandra / MySQL t·ª´ Docker Hub.
* `entrypoint.sh` ch·∫°y ETL t·ª± ƒë·ªông khi container start.

---

## 2. Docker Compose Setup
docker-compose:
```yaml
services:

  # Cassandra (Data Lake)
  cassandra:
    image: cassandra:4.1
    container_name: cassandra_dl
    ports:
      - "9042:9042"
    volumes:
      - cassandra-data:/var/lib/cassandra
    environment:
      CASSANDRA_CLUSTER_NAME: "first_cluster"
    healthcheck:
      test: ["CMD-SHELL", "cqlsh -e 'describe keyspaces'"]
      interval: 10s
      retries: 5
    networks:
      - de_project

  # MySQL (Data Warehouse)
  mysql:
    image: mysql:8.0.44-debian
    container_name: mysql_dwh
    ports:
      - "3307:3306"
    environment:
      MYSQL_ROOT_PASSWORD: 123
    volumes:
      - mysql-data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      retries: 5
    networks:
      - de_project

  # Spark Master
  spark-master:
    container_name: spark-engine
    build:
      context: .
      dockerfile: spark/Dockerfile
    image: da-spark-image
    entrypoint: ['./entrypoint.sh', 'master']
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - ../data:/opt/spark/data
      - ../etl:/opt/spark/etl
      - spark-logs:/opt/spark/spark-events
    env_file:
      - .env
    ports:
      - "9090:8080"  # Web UI
      - "7077:7077"  # Spark master
      - "4041:4040"  # Spark driver UI
    networks:
      - de_project

  # Grafana (Monitoring)
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    networks:
      - de_project

# Volumes
volumes:
  cassandra-data:
  mysql-data:
  spark-logs:

# Networks
networks:
  de_project:
    name: de_project
    driver: bridge
```

Gi·∫£i th√≠ch:

* Cassandra ‚Üí Data Lake, port 9042
* MySQL ‚Üí Data Warehouse, port 3307
* Spark Master ‚Üí ch·∫°y ETL, k·∫øt n·ªëi CSV/ETL code
* Grafana ‚Üí Monitoring, port 3000
* Volumes ‚Üí l∆∞u d·ªØ li·ªáu persistent
* Network de_project ‚Üí t·∫•t c·∫£ container c√πng network n·ªôi b·ªô

---

# II. ETL Pipeline
```
‚îú‚îÄ‚îÄ üêç Cassandra.py
‚îú‚îÄ‚îÄ üêç Main.py
‚îú‚îÄ‚îÄ üêç MySql.py
‚îî‚îÄ‚îÄ üìÑ generate_data_automatically.ipynb
```
M·ª•c ti√™u:
- L·∫•y d·ªØ li·ªáu th√¥ t·ª´ Cassandra (Datalake)
- Transfrom ·ªü Main
- ƒê∆∞a d·ªØ li·ªáu v√†o MySQL (Data Warehouse)
- T·∫°o c√°c b·∫£n ghi li√™n t·ª•c t·ª± ƒë·ªông ƒë∆∞a v√†o Datalake (üìÑ generate_data_automatically.ipynb)

### 1. Code
```python
import os
from uuid import UUID
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import StringType
from cassandra.util import datetime_from_uuid1

from Cassandra import Cassandra
from MySql import MySql

# ======================================================================
#                         SPARK CONFIGURATION
# ======================================================================

# ƒê∆∞·ªùng d·∫´n MySQL Driver (ƒë·ªÉ Spark c√≥ th·ªÉ ghi v√†o MySQL)
MYSQL_JAR = os.path.abspath("../driver/mysql-connector-j-8.0.33.jar")

# T·∫°o SparkSession, k√®m theo k·∫øt n·ªëi Cassandra + MySQL
spark = (
    SparkSession.builder.config(
        # Connector ƒë·ªÉ Spark ƒë·ªçc Cassandra
        "spark.jars.packages", "com.datastax.spark:spark-cassandra-connector_2.12:3.1.0"
    )
    # Add MySQL driver
    .config("spark.driver.extraClassPath", MYSQL_JAR)
    .config("spark.executor.extraClassPath", MYSQL_JAR)
    .getOrCreate()
)

# T·∫°o instance DB ƒë·ªÉ t∆∞∆°ng t√°c d·ªÖ d√†ng
cass = Cassandra(spark)
mysql = MySql(spark)

# ======================================================================
#                                UDF
# ======================================================================
# UDF n√†y d√πng ƒë·ªÉ l·∫•y timestamp th·∫≠t t·ª´ UUID v1 trong Cassandra
# Cassandra l∆∞u th·ªùi gian trong uuid1 ‚Üí ph·∫£i t·ª± convert
@udf(returnType=StringType())
def extract_timestamp_from_uuid(uuid_str):
    try:
        u = UUID(uuid_str)
        dt = datetime_from_uuid1(u)  # Tr√≠ch timestamp t·ª´ UUID
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except:
        return None


# ======================================================================
#                         DATA TRANSFORM PIPELINE
# ======================================================================
class DataTransformer:

    # Ch·ªâ nh·∫≠n c√°c event h·ª£p l·ªá
    valid_events = ["click", "conversion", "qualified", "unqualified"]

    # -------------------------------------------------------------
    # 1Ô∏è‚É£ Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
    # -------------------------------------------------------------
    @staticmethod
    def preprocess(df):
        # Chuy·ªÉn create_time (UUID) ‚Üí timestamp
        df = df.withColumn("system_ts", extract_timestamp_from_uuid(col("create_time")))
        df = df.withColumn("system_ts", to_timestamp("system_ts"))

        # Ch·ªçn c·ªôt c·∫ßn thi·∫øt ‚Äî lo·∫°i b·ªè c·ªôt r√°c
        return df.select(
            "create_time", "system_ts", "job_id", "custom_track", "bid",
            "campaign_id", "group_id", "publisher_id"
        ).filter("job_id IS NOT NULL AND custom_track IS NOT NULL")

    # -------------------------------------------------------------
    # 2Ô∏è‚É£ T·ªïng h·ª£p d·ªØ li·ªáu theo Job / ng√†y / gi·ªù / campaign
    # -------------------------------------------------------------
    @staticmethod
    def aggregate(df):
        # Ch·ªâ gi·ªØ event h·ª£p l·ªá
        df = df.filter(col("custom_track").isin(DataTransformer.valid_events))

        # Extract ng√†y & gi·ªù
        df = df.withColumn("dates", to_date("system_ts"))
        df = df.withColumn("hours", hour("system_ts"))

        # Pivot ƒë·ªÉ ƒë·∫øm m·ªói lo·∫°i event theo gi·ªù
        pivot_df = (
            df.groupBy("job_id", "dates", "hours", "publisher_id", "campaign_id", "group_id")
            .pivot("custom_track", DataTransformer.valid_events)
            .agg(count("*").alias("count"))
        )

        # ƒê·ªïi t√™n c·ªôt click_count ‚Üí click
        for e in DataTransformer.valid_events:
            pivot_df = pivot_df.withColumnRenamed(f"{e}_count", e)

        # T√≠nh spend theo gi·ªù + bid trung b√¨nh
        metric_df = df.groupBy("job_id", "publisher_id", "campaign_id", "group_id").agg(
            round(sum("bid"), 2).alias("spend_hour"),
            round(avg("bid"), 2).alias("bid_set"),
        )

        # JOIN l·∫°i ƒë·ªÉ c√≥ b·∫£ng ho√†n ch·ªânh
        return pivot_df.join(
            metric_df, ["job_id", "publisher_id", "campaign_id", "group_id"], "left"
        )

    # -------------------------------------------------------------
    # 3Ô∏è‚É£ ƒêi·ªÅn c√°c c·ªôt null th√†nh 0 ƒë·ªÉ d·ªÖ l∆∞u v√†o MySQL
    # -------------------------------------------------------------
    @staticmethod
    def fill_null(df):
        return df.fillna({
            "click": 0,
            "conversion": 0,
            "qualified": 0,
            "unqualified": 0,
            "spend_hour": 0,
            "bid_set": 0,
        })

    # -------------------------------------------------------------
    # 4Ô∏è‚É£ T·∫°o PK + timestamp c·∫≠p nh·∫≠t
    # -------------------------------------------------------------
    @staticmethod
    def post_process(df):
        df = df.withColumn("id", monotonically_increasing_id())
        df = df.withColumn("updated_at", current_timestamp())

        # Final schema cho b·∫£ng trong MySQL
        return df.select(
            "id", "job_id", "dates", "hours", "company_id", "group_id",
            "campaign_id", "publisher_id", "click", "conversion",
            "qualified", "unqualified", "bid_set", "spend_hour",
            "sources", "updated_at"
        )

    # -------------------------------------------------------------
    # 5Ô∏è‚É£ Pipeline ƒë·∫ßy ƒë·ªß
    # -------------------------------------------------------------
    @staticmethod
    def transform_full(df):
        df = DataTransformer.preprocess(df)
        df = DataTransformer.aggregate(df)
        df = df.withColumn("sources", lit("Cassandra"))
        df = DataTransformer.fill_null(df)

        # L·∫•y th√™m th√¥ng tin c√¥ng ty t·ª´ b·∫£ng job
        job_df = mysql.read("job").select(col("id").alias("job_id"), "company_id")
        df = df.join(job_df, "job_id", "left")

        return DataTransformer.post_process(df)


# ======================================================================
#                     CHECK SYNC MYSQL <-> CASSANDRA
# ======================================================================
class DataSync:

    # L·∫•y timestamp update cu·ªëi c√πng trong MySQL
    @staticmethod
    def last_mysql_date():
        df = mysql.read("campaign")
        return df.select(max("updated_at")).first()[0]

    # L·∫•y timestamp m·ªõi nh·∫•t t·ª´ Cassandra
    @staticmethod
    def last_cassandra_date():
        df = cass.read("tracking")
        df = df.withColumn("create_time", extract_timestamp_from_uuid("create_time"))
        df = df.withColumn("create_time", to_timestamp("create_time"))
        df = df.withColumn(
            "create_time",
            from_utc_timestamp("create_time", "Asia/Ho_Chi_Minh")
        )
        return df.select(max("create_time")).first()[0]


# ======================================================================
#                             MAIN ETL
# ======================================================================
def run_etl():
    df = cass.read("tracking")
    df = DataTransformer.transform_full(df)
    mysql.insert("campaign", df)


# ======================================================================
#                           ENTRY POINT
# ======================================================================
if __name__ == "__main__":
    # Load d·ªØ li·ªáu m·∫´u v√†o DB
    print("Insert Cassandra")
    cass.insert("tracking", spark_read_file("../data/cassandra/tracking.csv"))

    print("Insert MySQL")
    mysql.insert("job", spark_read_file("../data/mysql/job.csv"))

    # Ch·∫°y ETL l·∫ßn ƒë·∫ßu
    run_etl()

    # Worker ch·∫°y li√™n t·ª•c ‚Üí ch·ªâ ETL khi c√≥ d·ªØ li·ªáu m·ªõi
    while True:
        if DataSync.last_mysql_date() < DataSync.last_cassandra_date():
            run_etl()
```
### 2. K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c 
![ETL Demo](image/demo_etl.gif)

Nh·∫≠n x√©t: 
- D·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω t·ª± ƒë·ªông khi c√≥ b·∫£n ghi m·ªõi ƒë∆∞·ª£c ƒë∆∞a v√†o Datalake: Micro-Batch ETL

# III. Visualization (Grafana)
### 1. Config Mysql
```
Grafana ‚Üí Connections ‚Üí Data Sources ‚Üí mysql 
```

![Grafana Config](image/grafana_config.png)

### 2. K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c
![alt text](image/grafana.png)


# IV. Server Preparation

### 1. C√†i ƒë·∫∑t VM

* C√†i VirtualBox: [VirtualBox](https://www.virtualbox.org/wiki/Downloads)
* T·∫£i Ubuntu Server: [Ubuntu](https://ubuntu.com/download/server)

---

### 2. C·∫•u h√¨nh server

#### 2.1 T·∫°o m√°y ·∫£o

1. VirtualBox ‚Üí New ‚Üí Name: `Ubuntu_VM`
2. Type: Linux, Version: Ubuntu (64-bit)
3. RAM: 2‚Äì4 GB
4. Hard disk: 20GB+
5. Start VM ‚Üí c√†i Ubuntu t·ª´ ISO

#### 2.2 C·∫•u h√¨nh m·∫°ng

![Config Image](image/image.png)

* Settings ‚Üí Network ‚Üí Adapter 1 ‚Üí Port Forwarding
* M·ªü port SSH host ‚Üí VM (v√≠ d·ª• host port 2222 ‚Üí guest port 22)

#### 2.3 C√†i OpenSSH server

```bash
sudo apt update
sudo apt install openssh-server -y
sudo systemctl enable ssh
sudo systemctl start ssh
```

* Ki·ªÉm tra SSH t·ª´ host:

```bash
ssh <username>@<host_ip> -p <host_port>
```

![test\_ssh.png](image/test_ssh.png)

#### 2.4 C√†i ƒë·∫∑t h·ªó tr·ª£

```bash
sudo apt install git -y
sudo apt install docker.io docker-compose -y
```

# V. Deployment & CI/CD Pipeline
M·ª•c ƒë√≠ch: k√©o code t·ª´ github v·ªÅ server v√† ch·∫°y ·ªü server

```
git clone https://github.com/sonvuuu28/DE_2025_Recruitment_Pipeline.git
```
![git clone](image/gitclone.png)

```
sudo docker-compose up -d
```

![docker-compose up](image/docker-compose_up.png)











![alt text](image/docker_server.png)